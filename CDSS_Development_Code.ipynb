{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ftcl_i7KlFo",
        "outputId": "aeca30a4-f90c-4432-ef00-8d3b189cafe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.10/dist-packages (2.9.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install psycopg2-binary\n",
        "!pip install -qU llama-index==0.9.29\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb2Jvfh-K36O",
        "outputId": "9c613338-1cef-4c50-b586-822aafb42e66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psycopg2\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import os\n",
        "import chardet"
      ],
      "metadata": {
        "id": "lilmUh6FzfbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accessing the Database"
      ],
      "metadata": {
        "id": "e8-Hyr28K70i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ngrok-provided settings\n",
        "host = \"\"       # ngrok host from the forwarding URL\n",
        "port = \"\"                # ngrok port from the forwarding URL\n",
        "\n",
        "# PostgreSQL credentials\n",
        "user = \"\"\n",
        "password = \"\"\n",
        "database = \"\"\n",
        "\n",
        "\n",
        "# Connect to PostgreSQL\n",
        "try:\n",
        "    conn = psycopg2.connect(\n",
        "        host=host,\n",
        "        port=port,\n",
        "        user=user,\n",
        "        password=password,\n",
        "        dbname=database\n",
        "    )\n",
        "    print(\"PSQL Connection successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error connecting to database: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)  # Show all columns\n",
        "pd.set_option('display.width', None)  # Adjust width to avoid line wrapping\n",
        "pd.set_option('display.max_colwidth', None)  # Display full content in each cell\n",
        "\n",
        "\n",
        "# Execute Query\n",
        "def exec_query(query):\n",
        "    try:\n",
        "        # Establish the connection\n",
        "        conn = psycopg2.connect(\n",
        "        host=host,\n",
        "        port=port,\n",
        "        user=user,\n",
        "        password=password,\n",
        "        dbname=database\n",
        "        )\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Execute the query\n",
        "        cursor.execute(query)\n",
        "\n",
        "        # Fetch column names from the cursor\n",
        "        colnames = [desc[0] for desc in cursor.description]\n",
        "\n",
        "        # Fetch all rows of the query\n",
        "        results = cursor.fetchall()\n",
        "\n",
        "        # Create a DataFrame with column names\n",
        "        df = pd.DataFrame(results, columns=colnames)\n",
        "\n",
        "        # Close the cursor and connection\n",
        "        cursor.close()\n",
        "        conn.close()\n",
        "\n",
        "        # Return the DataFrame\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error executing query: {e}\")\n",
        "        # Roll back the transaction in case of error\n",
        "        conn.rollback()\n",
        "        cursor.close()\n",
        "        conn.close()\n",
        "        return None"
      ],
      "metadata": {
        "id": "mu4NF0ctK33t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Views from the complete data"
      ],
      "metadata": {
        "id": "2odmJvG8Lnk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_tables_query = \"\"\"\n",
        "SELECT table_name\n",
        "FROM information_schema.tables\n",
        "WHERE table_schema = 'public';\n",
        "\"\"\"\n",
        "\n",
        "list_views_query = \"\"\"\n",
        "SELECT table_name\n",
        "FROM information_schema.views\n",
        "WHERE table_schema = 'public';\n",
        "\"\"\"\n",
        "\n",
        "exec_query(list_views_query)\n",
        "\n",
        "\n",
        "# 1. Patient Demographics and Admissions\n",
        "query1 = \"\"\"\n",
        "CREATE OR REPLACE VIEW admissions_view AS\n",
        "SELECT DISTINCT\n",
        "    p.subject_id,\n",
        "    p.gender,\n",
        "    p.anchor_age,\n",
        "    p.anchor_year_group,\n",
        "    p.dod AS date_of_death,\n",
        "    a.hadm_id,\n",
        "    a.admission_type,\n",
        "    a.race,\n",
        "    a.hospital_expire_flag\n",
        "FROM\n",
        "    patients AS p\n",
        "INNER JOIN\n",
        "    admissions AS a ON p.subject_id = a.subject_id;\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# 2. Patient Diagnoses\n",
        "query2 = \"\"\"\n",
        "CREATE OR REPLACE VIEW diagnoses_view AS\n",
        "SELECT DISTINCT\n",
        "    d.subject_id,\n",
        "    d.hadm_id,\n",
        "    d.icd_code,\n",
        "    dicd.long_title AS diagnosis_description\n",
        "FROM\n",
        "    diagnoses_icd AS d\n",
        "INNER JOIN\n",
        "    d_icd_diagnoses AS dicd\n",
        "    ON d.icd_code = dicd.icd_code AND d.icd_version = dicd.icd_version\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# 3. Patient Procedures\n",
        "query3 = \"\"\"\n",
        "CREATE OR REPLACE VIEW procedures_view AS\n",
        "SELECT DISTINCT\n",
        "    pr.subject_id,\n",
        "    pr.hadm_id,\n",
        "    pr.icd_code,\n",
        "    dicp.long_title AS procedure_description\n",
        "FROM\n",
        "    procedures_icd AS pr\n",
        "INNER JOIN\n",
        "    d_icd_procedures AS dicp\n",
        "    ON pr.icd_code = dicp.icd_code AND pr.icd_version = dicp.icd_version\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# 4. Medication Orders\n",
        "query4 = \"\"\"\n",
        "CREATE OR REPLACE VIEW medications_view AS\n",
        "SELECT DISTINCT\n",
        "    pr.subject_id,\n",
        "    pr.hadm_id,\n",
        "    pr.drug,\n",
        "    pr.dose_val_rx AS dose_value,\n",
        "    pr.dose_unit_rx AS dose_unit,\n",
        "    pr.route\n",
        "FROM\n",
        "    prescriptions AS pr\n",
        "WHERE\n",
        "    pr.subject_id IS NOT NULL\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# 5. Laboratory Results\n",
        "query5 = \"\"\"\n",
        "CREATE OR REPLACE VIEW lab_results_view AS\n",
        "SELECT DISTINCT\n",
        "    le.subject_id,\n",
        "    le.hadm_id,\n",
        "    dl.label AS test_name,\n",
        "    le.valuenum AS test_value,\n",
        "    le.valueuom AS test_unit,\n",
        "    le.flag AS abnormal_flag\n",
        "FROM\n",
        "    labevents AS le\n",
        "INNER JOIN\n",
        "    d_labitems AS dl ON le.itemid = dl.itemid\n",
        "WHERE\n",
        "    le.valuenum IS NOT NULL\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# 6. Microbiology Results\n",
        "query7 = \"\"\"\n",
        "CREATE OR REPLACE VIEW microbiology_results_view AS\n",
        "SELECT DISTINCT\n",
        "    me.subject_id,\n",
        "    me.hadm_id,\n",
        "    me.spec_type_desc AS specimen_type,\n",
        "    me.test_name,\n",
        "    me.org_name AS organism_name,\n",
        "    me.ab_name AS antibiotic_name,\n",
        "    me.interpretation,\n",
        "    me.comments\n",
        "FROM\n",
        "    microbiologyevents AS me\n",
        "WHERE\n",
        "    me.subject_id IS NOT NULL\n",
        "    AND me.hadm_id IS NOT NULL\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# List of view names\n",
        "view_names = [\n",
        "    'medications_view',\n",
        "    'lab_results_view',\n",
        "    'admissions_view',\n",
        "    'diagnoses_view',\n",
        "    'procedures_view',\n",
        "    'microbiology_results_view'\n",
        "]\n",
        "# Loop through each view and display 5 records\n",
        "for view in view_names:\n",
        "    query = f\"SELECT * FROM {view} LIMIT 5;\"\n",
        "    df = exec_query(query)\n",
        "    print(f\" {view}:\")\n",
        "    display(df)\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zc_Bi11NK31T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structured Data -> Semi-Structured JSON"
      ],
      "metadata": {
        "id": "ecfIQ-7VozfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ngrok-provided settings\n",
        "host = \"\"       # ngrok host from the forwarding URL\n",
        "port = \"\"                # ngrok port from the forwarding URL\n",
        "\n",
        "# PostgreSQL credentials\n",
        "user = \"\"\n",
        "password = \"\"\n",
        "database = \"\"\n",
        "\n",
        "\n",
        "# Establish the connection\n",
        "conn = psycopg2.connect(\n",
        "        host=host,\n",
        "        port=port,\n",
        "        user=user,\n",
        "        password=password,\n",
        "        dbname=database\n",
        "    )\n",
        "\n",
        "# Initialize the cursor\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Define queries for each view\n",
        "queries = {\n",
        "    \"admissions_view\": \"SELECT * FROM admissions_view;\",\n",
        "    \"diagnoses_view\": \"SELECT * FROM diagnoses_view;\",\n",
        "    \"procedures_view\": \"SELECT * FROM procedures_view;\",\n",
        "    \"medications_view\": \"SELECT * FROM medications_view;\",\n",
        "    \"lab_results_view\": \"SELECT * FROM lab_results_view;\",\n",
        "    \"microbiology_results_view\": \"SELECT * FROM microbiology_results_view;\"\n",
        "}\n",
        "\n",
        "# Initialize a dictionary to hold unstructured data by subject_id\n",
        "data_by_subject_id = {}\n",
        "\n",
        "# Process each view and aggregate data by subject_id and hadm_id\n",
        "for view_name, query in queries.items():\n",
        "    cursor.execute(query)\n",
        "    columns = [desc[0] for desc in cursor.description]  # Column names\n",
        "    rows = cursor.fetchall()\n",
        "\n",
        "    for row in rows:\n",
        "        record = dict(zip(columns, row))  # Convert row to dictionary\n",
        "        subject_id = record[\"subject_id\"]\n",
        "        hadm_id = record[\"hadm_id\"]\n",
        "\n",
        "        if subject_id not in data_by_subject_id:\n",
        "            data_by_subject_id[subject_id] = {\n",
        "                \"sr_id\": None,  # Will be assigned later\n",
        "                \"hadm_ids\": {}\n",
        "            }\n",
        "\n",
        "        # Initialize structure for hadm_id within the subject\n",
        "        if hadm_id not in data_by_subject_id[subject_id][\"hadm_ids\"]:\n",
        "            data_by_subject_id[subject_id][\"hadm_ids\"][hadm_id] = {\n",
        "                \"admissions\": {},\n",
        "                \"medications\": [],\n",
        "                \"lab_results\": [],\n",
        "                \"diagnoses\": [],\n",
        "                \"procedures\": [],\n",
        "                \"microbiology_results\": []\n",
        "            }\n",
        "\n",
        "        # Organize records based on view into appropriate fields\n",
        "        if view_name == \"admissions_view\":\n",
        "            data_by_subject_id[subject_id][\"hadm_ids\"][hadm_id][\"admissions\"].update({\n",
        "                \"gender\": record[\"gender\"],\n",
        "                \"anchor_age\": record[\"anchor_age\"],\n",
        "                \"anchor_year_group\": record[\"anchor_year_group\"],\n",
        "                \"date_of_death\": record[\"date_of_death\"],\n",
        "                \"admission_type\": record[\"admission_type\"],\n",
        "                \"race\": record[\"race\"],\n",
        "                \"hospital_expire_flag\": record[\"hospital_expire_flag\"]\n",
        "            })\n",
        "        elif view_name == \"diagnoses_view\":\n",
        "            data_by_subject_id[subject_id][\"hadm_ids\"][hadm_id][\"diagnoses\"].append({\n",
        "                \"icd_code\": record[\"icd_code\"],\n",
        "                \"description\": record[\"diagnosis_description\"]\n",
        "            })\n",
        "        elif view_name == \"procedures_view\":\n",
        "            data_by_subject_id[subject_id][\"hadm_ids\"][hadm_id][\"procedures\"].append({\n",
        "                \"icd_code\": record[\"icd_code\"],\n",
        "                \"description\": record[\"procedure_description\"]\n",
        "            })\n",
        "        elif view_name == \"medications_view\":\n",
        "            data_by_subject_id[subject_id][\"hadm_ids\"][hadm_id][\"medications\"].append({\n",
        "                \"drug\": record[\"drug\"],\n",
        "                \"dose\": record[\"dose_value\"],\n",
        "                \"dose_unit\": record[\"dose_unit\"],\n",
        "                \"route\": record[\"route\"]\n",
        "            })\n",
        "        elif view_name == \"lab_results_view\":\n",
        "            data_by_subject_id[subject_id][\"hadm_ids\"][hadm_id][\"lab_results\"].append({\n",
        "                \"test_name\": record[\"test_name\"],\n",
        "                \"test_value\": record[\"test_value\"],\n",
        "                \"test_unit\": record[\"test_unit\"],\n",
        "                \"abnormal_flag\": record[\"abnormal_flag\"]\n",
        "            })\n",
        "        elif view_name == \"microbiology_results_view\":\n",
        "            data_by_subject_id[subject_id][\"hadm_ids\"][hadm_id][\"microbiology_results\"].append({\n",
        "                \"specimen_type\": record[\"specimen_type\"],\n",
        "                \"test_name\": record[\"test_name\"],\n",
        "                \"organism_name\": record.get(\"organism_name\"),\n",
        "                \"antibiotic_name\": record.get(\"antibiotic_name\"),\n",
        "                \"interpretation\": record.get(\"interpretation\"),\n",
        "                \"comments\": record[\"comments\"]\n",
        "            })\n",
        "\n",
        "cursor.close()\n",
        "conn.close()\n",
        "\n",
        "# Initialize the sr_id counter\n",
        "sr_id_counter = 1001\n",
        "\n",
        "# Prepare the final unstructured data\n",
        "semistructured_data = []\n",
        "\n",
        "# Iterate over each subject_id to convert data into the desired JSON format\n",
        "for subject_id, subject_data in tqdm(data_by_subject_id.items(), desc=\"Processing subjects\"):\n",
        "    # Assign a unique sr_id to each subject\n",
        "    subject_data[\"sr_id\"] = sr_id_counter\n",
        "    sr_id_counter += 1  # Increment the unique id for each entry\n",
        "\n",
        "    # Append the structured data to the final unstructured data list\n",
        "    semistructured_data.append({\n",
        "        \"sr_id\": subject_data[\"sr_id\"],\n",
        "        \"subject_id\": subject_id,\n",
        "        \"hadm_ids\": subject_data[\"hadm_ids\"]\n",
        "    })\n",
        "\n",
        "\n",
        "# Save to JSON file\n",
        "output_file = \"/content/drive/MyDrive/Research/PSQL-MIMIC-IV/MIMIC-IV-Semi-Structured-Data-Hierarchical.json\"\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump(semistructured_data, f, indent=2)"
      ],
      "metadata": {
        "id": "Exmk7tr_K3y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reducing the JSON"
      ],
      "metadata": {
        "id": "sOZ1gEV1s2fK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REDUCING JSON\n",
        "\n",
        "# Mount Google Drive (if not already mounted)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the input JSON file\n",
        "input_file = \"/content/drive/MyDrive/Research/PSQL-MIMIC-IV/MIMIC-IV-Semi-Structured-Data-Hierarchical.json\"\n",
        "\n",
        "# Path to the output JSON file\n",
        "output_file = \"/content/drive/MyDrive/Research/PSQL-MIMIC-IV/MIMIC-IV-Semi-Structured-Data-Hierarchical-Reduced.json\"\n",
        "\n",
        "# Read the JSON file\n",
        "with open(input_file, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Iterate over the data and modify as per the requirements\n",
        "for record in data:\n",
        "    # For each record, get hadm_ids and limit to 5 hadm_ids\n",
        "    hadm_ids = record.get('hadm_ids', {})\n",
        "    # Convert hadm_ids to a list of tuples (hadm_id, hadm_data)\n",
        "    hadm_items = list(hadm_ids.items())\n",
        "    # Limit to first 5 hadm_ids\n",
        "    limited_hadm_items = hadm_items[:5]\n",
        "    # Create a new hadm_ids dict with limited hadm_ids\n",
        "    new_hadm_ids = {}\n",
        "    for hadm_id, hadm_data in limited_hadm_items:\n",
        "        # Truncate medications to first 5 records\n",
        "        if 'medications' in hadm_data:\n",
        "            hadm_data['medications'] = hadm_data['medications'][:5]\n",
        "        # Truncate lab_results to first 5 records\n",
        "        if 'lab_results' in hadm_data:\n",
        "            hadm_data['lab_results'] = hadm_data['lab_results'][:5]\n",
        "        # Add the modified hadm_data back to new_hadm_ids\n",
        "        new_hadm_ids[hadm_id] = hadm_data\n",
        "    # Update the record's hadm_ids with the new limited hadm_ids\n",
        "    record['hadm_ids'] = new_hadm_ids\n",
        "\n",
        "# Save the modified data back to JSON\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump(data, f, indent=2)\n",
        "\n",
        "print(f\"Reduced JSON saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRFS55EAK3wQ",
        "outputId": "69079506-934f-4eaa-bcab-8bcdbbab496a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Reduced JSON saved to /content/drive/MyDrive/Research/PSQL-MIMIC-IV/MIMIC-IV-Semi-Structured-Data-Hierarchical-Reduced.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JSON -> Text for Indexing"
      ],
      "metadata": {
        "id": "Cya11YvGyGQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load your hierarchical JSON data\n",
        "input_file = \"/content/drive/MyDrive/Research/PSQL-MIMIC-IV/MIMIC-IV-Semi-Structured-Data-Hierarchical-Reduced.json\"\n",
        "with open(input_file, 'r') as f:\n",
        "    unstructured_data = json.load(f)\n",
        "\n",
        "# Function to convert each patient's data to a structured text format\n",
        "def convert_to_text(data):\n",
        "    text_data = []\n",
        "    for entry in tqdm(data, desc=\"Processing entries\"):\n",
        "        subject_text = f\"Subject ID: {entry['subject_id']} (SR ID: {entry['sr_id']})\\n\"\n",
        "\n",
        "        for hadm_id, hadm_data in entry[\"hadm_ids\"].items():\n",
        "            hadm_text = f\"\\n---\\n\\nAdmission ID: {hadm_id}\\n\"\n",
        "\n",
        "            # Admission details\n",
        "            if hadm_data[\"admissions\"]:\n",
        "                admission = hadm_data[\"admissions\"]\n",
        "                admission_text = (\n",
        "                    f\"Admission Details:\\n\"\n",
        "                    f\"  1. Gender: {admission['gender']}\\n\"\n",
        "                    f\"  2. Age: {admission['anchor_age']}\\n\"\n",
        "                    f\"  3. Admission Year Group: {admission['anchor_year_group']}\\n\"\n",
        "                    f\"  4. Admission Type: {admission['admission_type']}\\n\"\n",
        "                    f\"  5. Race: {admission['race']}\\n\"\n",
        "                    f\"  6. Expired in Hospital: {'Yes' if admission['hospital_expire_flag'] == '1' else 'No'}\\n\"\n",
        "                    f\"  7. Date of Death: {admission['date_of_death'] if admission['date_of_death'] else 'Not Applicable'}\\n\"\n",
        "                )\n",
        "                hadm_text += admission_text\n",
        "\n",
        "            # Medications\n",
        "            medications = hadm_data[\"medications\"]\n",
        "            if medications:\n",
        "                medication_text = \"Medications:\\n\"\n",
        "                for i, med in enumerate(medications, start=1):\n",
        "                    medication_text += (\n",
        "                        f\"  {i}. {med['drug']}\\n\"\n",
        "                        f\"     - Dose: {med['dose']} {med['dose_unit']}\\n\"\n",
        "                        f\"     - Route: {med['route']}\\n\"\n",
        "                    )\n",
        "                hadm_text += medication_text\n",
        "\n",
        "            # Lab Results\n",
        "            lab_results = hadm_data[\"lab_results\"]\n",
        "            if lab_results:\n",
        "                lab_text = \"Lab Results:\\n\"\n",
        "                for i, lab in enumerate(lab_results, start=1):\n",
        "                    abnormality = \" (Abnormal)\" if lab[\"abnormal_flag\"] == \"abnormal\" else \"\"\n",
        "                    lab_text += (\n",
        "                        f\"  {i}. {lab['test_name']}\\n\"\n",
        "                        f\"     - Value: {lab['test_value']} {lab['test_unit']}{abnormality}\\n\"\n",
        "                    )\n",
        "                hadm_text += lab_text\n",
        "\n",
        "            # Diagnoses\n",
        "            diagnoses = hadm_data[\"diagnoses\"]\n",
        "            if diagnoses:\n",
        "                diagnosis_text = \"Diagnoses:\\n\"\n",
        "                for i, diag in enumerate(diagnoses, start=1):\n",
        "                    diagnosis_text += (\n",
        "                        f\"  {i}. ICD Code {diag['icd_code']}\\n\"\n",
        "                        f\"     - Description: {diag['description']}\\n\"\n",
        "                    )\n",
        "                hadm_text += diagnosis_text\n",
        "\n",
        "            # Procedures\n",
        "            procedures = hadm_data[\"procedures\"]\n",
        "            if procedures:\n",
        "                procedure_text = \"Procedures:\\n\"\n",
        "                for i, proc in enumerate(procedures, start=1):\n",
        "                    procedure_text += (\n",
        "                        f\"  {i}. ICD Code {proc['icd_code']}\\n\"\n",
        "                        f\"     - Description: {proc['description']}\\n\"\n",
        "                    )\n",
        "                hadm_text += procedure_text\n",
        "\n",
        "            # Microbiology Results\n",
        "            micro_results = hadm_data[\"microbiology_results\"]\n",
        "            if micro_results:\n",
        "                micro_text = \"Microbiology Results:\\n\"\n",
        "                for i, micro in enumerate(micro_results, start=1):\n",
        "                    comments = f\" (Comments: {micro['comments']})\" if micro[\"comments\"] else \"\"\n",
        "                    micro_text += (\n",
        "                        f\"  {i}. Specimen: {micro['specimen_type']}\\n\"\n",
        "                        f\"     - Test: {micro['test_name']}\\n\"\n",
        "                        f\"     - Interpretation: {micro['interpretation'] if micro['interpretation'] else 'Not Provided'}\\n\"\n",
        "                        f\"     {comments}\\n\"\n",
        "                    )\n",
        "                hadm_text += micro_text\n",
        "\n",
        "            subject_text += hadm_text\n",
        "\n",
        "        # Append the final structured text for this subject\n",
        "        text_data.append(subject_text)\n",
        "    return text_data\n",
        "\n",
        "# Convert the unstructured data to structured text format\n",
        "structured_text_data = convert_to_text(unstructured_data)\n",
        "\n",
        "# Save to a text file for indexing with LlamaIndex\n",
        "output_file = \"/content/drive/MyDrive/Research/PSQL-MIMIC-IV/MIMIC-IV-Structured-Text-Data-Reduced.txt\"\n",
        "with open(output_file, 'w') as f:\n",
        "    for text_entry in structured_text_data:\n",
        "        f.write(text_entry + \"\\n\\n\")  # Add separation between entries\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiBq00d8K3tq",
        "outputId": "f896ea23-2ac6-42f4-8e9f-91a2443a5b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing entries: 100%|██████████| 100/100 [00:00<00:00, 7568.21it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Indexes with VectorStoreIndex"
      ],
      "metadata": {
        "id": "7WGJPFAezbxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/Research/PSQL-MIMIC-IV/MIMIC-IV-Structured-Text-Data-Reduced.txt', 'rb') as file:\n",
        "    raw_data = file.read()\n",
        "\n",
        "result = chardet.detect(raw_data)\n",
        "file_encoding = result['encoding']\n",
        "print(file_encoding)\n",
        "\n",
        "# Use the detected encoding\n",
        "with open('/content/drive/MyDrive/Research/PSQL-MIMIC-IV/MIMIC-IV-Structured-Text-Data-Reduced.txt', 'r', encoding=file_encoding) as file:\n",
        "    big_text = file.read()\n",
        "\n",
        "import os\n",
        "from llama_index import VectorStoreIndex, Document\n",
        "\n",
        "# Set OpenAI API Key\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "\n",
        "# Load and split your encyclopedia text\n",
        "sections = big_text.split('\\n\\n')  # Split by paragraphs or sections\n",
        "\n",
        "# Filter out empty sections and create Document objects\n",
        "documents = [Document(text=section) for section in sections if section.strip()]\n",
        "\n",
        "# Check if documents list is empty after filtering\n",
        "if not documents:\n",
        "    raise ValueError(\"No content to index. Please ensure 'big_text' has content.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87Pfn6SnK3q7",
        "outputId": "8708907b-b845-47f6-ac34-37b407baa1a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ISO-8859-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Storing Indexes"
      ],
      "metadata": {
        "id": "LCEhewzp0QN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the directory exists\n",
        "\n",
        "persist_dir = \"/content/drive/MyDrive/Research/PSQL-MIMIC-IV/VectorStore_Indexes\"\n",
        "os.makedirs(persist_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "-qpuT8xuK3oE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build your index with the default storage context\n",
        "from llama_index import VectorStoreIndex\n",
        "\n",
        "# Assume 'documents' is your list of Document objects\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# Save the index to the specified directory\n",
        "index.storage_context.persist(persist_dir=persist_dir)\n",
        "\n",
        "print(f\"Index successfully saved to {persist_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7vDUcrw0SVv",
        "outputId": "983428ed-e865-4d0f-c49e-2a8219582233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index successfully saved to /content/drive/MyDrive/Research/PSQL-MIMIC-IV/VectorStore_Indexes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Indexes to use them"
      ],
      "metadata": {
        "id": "lTIeEMO40o7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD INDEXES\n",
        "\n",
        "# Import necessary libraries\n",
        "from llama_index import StorageContext, load_index_from_storage\n",
        "\n",
        "\n",
        "# Set OpenAI API Key\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "\n",
        "# Specify the storage directory\n",
        "persist_dir = \"/content/drive/MyDrive/Research/PSQL-MIMIC-IV/VectorStore_Indexes\"\n",
        "\n",
        "# Initialize the storage context with the persist_dir\n",
        "storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
        "\n",
        "# Load the index from the storage context\n",
        "index = load_index_from_storage(storage_context)\n",
        "\n",
        "print(\"Index successfully loaded from storage.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKag9Bim0YYp",
        "outputId": "c81358ad-5972-49b8-f98c-ac9af843bd18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index successfully loaded from storage.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample Queries"
      ],
      "metadata": {
        "id": "zXT_jd-q0xyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a query engine from the index\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "# Perform a query\n",
        "response = query_engine.query(\"Which patients received an intravenous dose of Potassium Chloride, and what were their corresponding diagnoses?\")\n",
        "\n",
        "# Print the response\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNiatgC50szp",
        "outputId": "e5a3f72c-3253-4c57-f842-4c29c1799c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patient with Admission ID 21133938 received an intravenous dose of Potassium Chloride. The corresponding diagnoses for this patient were Hyperkalemia and Acute and subacute hepatic failure without coma.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a query\n",
        "response = query_engine.query(\"Which patients received a transfusion of 0.9% Sodium Chloride, and what were their vital lab test results post-administration?\")\n",
        "\n",
        "# Print the response\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etyJJRcAis-X",
        "outputId": "52e49003-1bd8-42d1-d175-ba1e52912d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patient with Admission ID 27703517 received a transfusion of 0.9% Sodium Chloride. The vital lab test results post-administration were as follows:\n",
            "- Eosinophils: 2.7%\n",
            "- MCHC: 33.3%\n",
            "- MCV: 83 fL\n",
            "- Platelet Count: 335 K/uL\n",
            "- Anion Gap: 16 mEq/L\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a query\n",
        "response = query_engine.query(\"Identify patients with a diagnosis of portal hypertension and the procedures they underwent.\")\n",
        "\n",
        "# Print the response\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbYW7Uc8is2T",
        "outputId": "8004e242-1845-4a90-a284-8cd2d3864dc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patient with Admission ID 21636229 has a diagnosis of portal hypertension. The procedure they underwent was Drainage of Peritoneal Cavity, Percutaneous Approach, Diagnostic.\n",
            "\n",
            "Patient with Admission ID 23514107 also has a diagnosis of portal hypertension. No specific procedure related to portal hypertension was mentioned in the provided context information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a query\n",
        "response = query_engine.query(\"how to treat stage 3 pancreatic cancer\")\n",
        "\n",
        "# Print the response\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtxwXPJymUq8",
        "outputId": "950d9da5-7d8f-4970-921b-ce3a5427abc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Surgery, chemotherapy, and radiation therapy are common treatments for stage 3 pancreatic cancer. In some cases, a combination of these treatments may be recommended to help manage the disease. It is important for patients to consult with their healthcare team to determine the most appropriate treatment plan based on their individual circumstances.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gb232J29meg3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}